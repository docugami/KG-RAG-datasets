{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Docugami KG-RAG against OpenAI Assistants Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Important: Create your OpenAI assistant via https://platform.openai.com/playground\n",
    "#            and put the assistant ID here. Make sure you upload the identical set of\n",
    "#            files listed below (these files will be uploaded automatically to Docugami)\n",
    "OPENAI_ASSISTANT_ID = \"asst_g837jjwr6Ohgk2EWfQOKTcPg\"\n",
    "\n",
    "DOCSET_NAME = \"Earnings Calls Evaluation 12-06-2023\"\n",
    "FILES_DIR = Path(os.getcwd()) / \"v1/docs\"\n",
    "FILE_NAMES = [\n",
    "    \"Q1 2022 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q1 2023 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q2 2022 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q2 2023 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q3 2021 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q3 2022 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q3 2023 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q4 2020 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q4 2022 Snowflake Inc. Earnings Call - Snowflake Inc - BamSEC.pdf\",\n",
    "    \"Q3 FY23 Microsoft Corp Earnings Call.pdf\",\n",
    "]\n",
    "GROUND_TRUTH_CSV = Path(os.getcwd()) / \"v1/ground-truth-earning_calls.csv\"\n",
    "\n",
    "# We will run each experiment multiple times and average, \n",
    "# since results vary slightly over runs\n",
    "PER_EXPERIMENT_RUN_COUNT = 5\n",
    "\n",
    "# Note: Please specify ~6 (or more!) similar files to process together as a document set\n",
    "# This is currently a requirement for Docugami to automatically detect motifs\n",
    "# across the document set to generate a semantic XML Knowledge Graph.\n",
    "assert len(FILE_NAMES) >= 6, \"Please provide at least 6 files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langsmith import Client\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "\n",
    "# Dataset\n",
    "client = Client()\n",
    "dataset_name = DOCSET_NAME\n",
    "existing_datasets = list(client.list_datasets(dataset_name=dataset_name))\n",
    "if existing_datasets:\n",
    "    # read existing dataset\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "else:\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    # Populate dataset\n",
    "    for _, row in df.iterrows():\n",
    "        q = row[\"Question\"]\n",
    "        a = row[\"Answer\"]\n",
    "        client.create_example(\n",
    "            inputs={\"question\": q}, outputs={\"answer\": a}, dataset_id=dataset.id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Docugami KG-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pip --quiet --upgrade\n",
    "! pip install docugami==0.0.9 dgml-utils==0.3.0 --quiet --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload files to Docugami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami import Docugami\n",
    "from docugami.lib.upload import upload_to_named_docset, wait_for_dgml\n",
    "\n",
    "dg_client = Docugami()\n",
    "file_paths = [FILES_DIR / file_name for file_name in FILE_NAMES]\n",
    "\n",
    "# Files will not be re-uploaded if they were previously uploaded (based on name)\n",
    "dg_docs = upload_to_named_docset(dg_client, file_paths, DOCSET_NAME)\n",
    "\n",
    "docset_id = \"\"\n",
    "docset_name = \"\"\n",
    "for doc in dg_docs:\n",
    "    if not docset_id:\n",
    "        docset_id = doc.docset.id\n",
    "    else:\n",
    "        # all docs must be in the same docset\n",
    "        assert docset_id == doc.docset.id\n",
    "\n",
    "    if not docset_name:\n",
    "        docset_name = dg_client.docsets.retrieve(doc.docset.id).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for files to finish processing (OCR, and zero-shot creation of XML knowledge graph)\n",
    "\n",
    "# Note: This can take some time on the free docugami tier (up to ~20 mins). Please contact us for faster paid plans.\n",
    "wait_for_dgml(dg_client, dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run indexing\n",
    "from docugami_kg_rag.helpers.indexing import index_docset\n",
    "\n",
    "assert docset_id\n",
    "assert docset_name\n",
    "\n",
    "# Note: This can take some time since it is embedding and creating summaries for all the docs and chunks\n",
    "index_docset(docset_id=docset_id, name=docset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docugami Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "from docugami_kg_rag.chain import agent as docugami_agent, _get_tools, AgentInput\n",
    "\n",
    "def predict_docugami_agent(input: dict) -> dict:\n",
    "    question = input[\"question\"]\n",
    "    chain = AgentExecutor(\n",
    "        agent=docugami_agent,\n",
    "        tools=_get_tools(),\n",
    "    ).with_types(\n",
    "        input_type=AgentInput,\n",
    "    )\n",
    "    result = chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"use_reports\": False,\n",
    "        \"chat_history\": [],\n",
    "    })\n",
    "\n",
    "    return result[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent to make sure it is working\n",
    "predict_docugami_agent({\"question\": \"What was the question from Barclays in the Q2 2023 earnings call?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OpenAI Assistants Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI Agent\n",
    "\n",
    "Please go to https://platform.openai.com/playground and create your agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.openai_assistant import OpenAIAssistantRunnable\n",
    "\n",
    "def predict_openai_agent(input: dict, config: dict = None) -> dict:\n",
    "    openai_agent = OpenAIAssistantRunnable(assistant_id=OPENAI_ASSISTANT_ID, as_agent=True).with_config(config)\n",
    "    question = input[\"question\"]\n",
    "    result = openai_agent.invoke({\"content\": question})\n",
    "\n",
    "    return result.return_values[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent to make sure it is working\n",
    "predict_openai_agent({\"question\": \"What was the question from Barclays in the Q2 2023 earnings call?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langsmith.client import Client\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langchain.globals import set_llm_cache, get_llm_cache\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"],\n",
    ")\n",
    "\n",
    "def run_eval(eval_func, eval_run_name):\n",
    "    \"\"\"\n",
    "    Run eval\n",
    "    \"\"\"\n",
    "    client = Client()\n",
    "    client.run_on_dataset(\n",
    "        dataset_name=DOCSET_NAME,\n",
    "        llm_or_chain_factory=eval_func,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=eval_run_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Experiments\n",
    "agent_map = {\n",
    "    \"openai_assistant_retrieval\": predict_openai_agent,\n",
    "    \"docugami_kg_rag_zero_shot\": predict_docugami_agent,\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Disable global cache setting to get fresh results every time for all experiments\n",
    "    # since no caching or temperature-0 is supported for the openai assistants API and\n",
    "    # we want to measure under similar conditions\n",
    "    cache = get_llm_cache()\n",
    "    set_llm_cache(None)\n",
    "\n",
    "    for i in range(PER_EXPERIMENT_RUN_COUNT):\n",
    "        run_id = str(uuid.uuid4())\n",
    "        for project_name, agent in agent_map.items():\n",
    "            run_eval(agent, project_name + \"_\" + run_id)\n",
    "finally:\n",
    "    # Revert cache setting to global default\n",
    "    set_llm_cache(cache)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-sMPCFT4i-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
