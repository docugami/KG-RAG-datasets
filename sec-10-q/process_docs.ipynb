{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Documents\n",
    "\n",
    "This notebooks processes the documents in this dataset and prepares the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DOCSET_NAME = \"SEC 10Q Filings\"\n",
    "DOCS_DIR = Path(os.getcwd()) / \"docs\"\n",
    "RAW_QUESTIONS_DIR = Path(os.getcwd()) / \"data/raw_questions\"\n",
    "DGML_DIR = Path(os.getcwd()) / \"dgml\"\n",
    "TEXT_DIR = Path(os.getcwd()) / \"text\"\n",
    "SIMPLIFIED_XML_DIR = Path(os.getcwd()) / \"simplified-xml\"\n",
    "\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(RAW_QUESTIONS_DIR, exist_ok=True)\n",
    "os.makedirs(DGML_DIR, exist_ok=True)\n",
    "os.makedirs(TEXT_DIR, exist_ok=True)\n",
    "os.makedirs(SIMPLIFIED_XML_DIR, exist_ok=True)\n",
    "\n",
    "FILE_NAMES = [\n",
    "    \"2022 Q3 AAPL.pdf\",\n",
    "    \"2022 Q3 AMZN.pdf\",\n",
    "    \"2022 Q3 INTC.pdf\",\n",
    "    \"2022 Q3 MSFT.pdf\",\n",
    "    \"2022 Q3 NVDA.pdf\",\n",
    "    \"2023 Q1 AAPL.pdf\",\n",
    "    \"2023 Q1 AMZN.pdf\",\n",
    "    \"2023 Q1 INTC.pdf\",\n",
    "    \"2023 Q1 MSFT.pdf\",\n",
    "    \"2023 Q1 NVDA.pdf\",\n",
    "    \"2023 Q2 AAPL.pdf\",\n",
    "    \"2023 Q2 AMZN.pdf\",\n",
    "    \"2023 Q2 INTC.pdf\",\n",
    "    \"2023 Q2 MSFT.pdf\",\n",
    "    \"2023 Q2 NVDA.pdf\",\n",
    "    \"2023 Q3 AAPL.pdf\",\n",
    "    \"2023 Q3 AMZN.pdf\",\n",
    "    \"2023 Q3 INTC.pdf\",\n",
    "    \"2023 Q3 MSFT.pdf\",\n",
    "    \"2023 Q3 NVDA.pdf\",\n",
    "]\n",
    "\n",
    "# Note: Please specify ~6 (or more!) similar files to process together as a document set\n",
    "# This is currently a requirement for Docugami to automatically detect motifs\n",
    "# across the document set to generate a semantic XML Knowledge Graph.\n",
    "assert len(FILE_NAMES) >= 6, \"Please provide at least 6 files\"\n",
    "\n",
    "QUESTIONS_CSV = RAW_QUESTIONS_DIR / \"questions.csv\"\n",
    "QUESTIONS_WITH_LLM_ANSWERS_CSV = RAW_QUESTIONS_DIR / \"questions_with_LLM_answers.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "LOCAL_LLM_CACHE_DB_FILE = os.environ.get(\"LOCAL_LLM_CACHE\", \"/tmp/docugami/.langchain.db\")\n",
    "os.makedirs(Path(LOCAL_LLM_CACHE_DB_FILE).parent, exist_ok=True)\n",
    "set_llm_cache(SQLiteCache(database_path=LOCAL_LLM_CACHE_DB_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docugami import Docugami\n",
    "from docugami.lib.upload import upload_to_named_docset, wait_for_dgml\n",
    "\n",
    "dg_client = Docugami()\n",
    "file_paths = [DOCS_DIR / file_name for file_name in FILE_NAMES]\n",
    "\n",
    "# Files will not be re-uploaded if they were previously uploaded (based on name)\n",
    "dg_docs = upload_to_named_docset(dg_client, file_paths, DOCSET_NAME)\n",
    "\n",
    "docset_id = \"\"\n",
    "docset_name = \"\"\n",
    "for doc in dg_docs:\n",
    "    if not docset_id:\n",
    "        docset_id = doc.docset.id\n",
    "    else:\n",
    "        # all docs must be in the same docset\n",
    "        assert docset_id == doc.docset.id\n",
    "\n",
    "    if not docset_name:\n",
    "        docset_name = dg_client.docsets.retrieve(doc.docset.id).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for files to finish processing (OCR, and zero-shot creation of XML knowledge graph)\n",
    "\n",
    "# Note: This can take some time on the free docugami tier (up to ~20 mins). Please contact us for faster paid plans.\n",
    "dgml_map = wait_for_dgml(dg_client, dg_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2022 Q3 AAPL.pdf': '/tmp/tmparn9q6fy',\n",
       " '2022 Q3 AMZN.pdf': '/tmp/tmpkboib076',\n",
       " '2022 Q3 INTC.pdf': '/tmp/tmpuml_wiez',\n",
       " '2022 Q3 MSFT.pdf': '/tmp/tmpwc4x0wep',\n",
       " '2023 Q1 AAPL.pdf': '/tmp/tmpy21juqwa',\n",
       " '2023 Q1 AMZN.pdf': '/tmp/tmpja08i3h0',\n",
       " '2023 Q1 INTC.pdf': '/tmp/tmpzwuu6lcw',\n",
       " '2023 Q1 MSFT.pdf': '/tmp/tmpt5mepyeo',\n",
       " '2023 Q1 NVDA.pdf': '/tmp/tmpe0vz2xif',\n",
       " '2023 Q2 AAPL.pdf': '/tmp/tmpanzu7tfj',\n",
       " '2023 Q2 AMZN.pdf': '/tmp/tmpy3lbp4pu',\n",
       " '2023 Q2 INTC.pdf': '/tmp/tmpcxf55wli',\n",
       " '2023 Q2 MSFT.pdf': '/tmp/tmpmrz1llpv',\n",
       " '2023 Q2 NVDA.pdf': '/tmp/tmpeacawyez',\n",
       " '2023 Q3 AAPL.pdf': '/tmp/tmpmaa1utjq',\n",
       " '2023 Q3 AMZN.pdf': '/tmp/tmpiuab1mcu',\n",
       " '2023 Q3 INTC.pdf': '/tmp/tmpth0ubtpv',\n",
       " '2023 Q3 MSFT.pdf': '/tmp/tmpgn1v4whe',\n",
       " '2023 Q3 NVDA.pdf': '/tmp/tmpp29vq6e_',\n",
       " '2022 Q3 NVDA.pdf': '/tmp/tmp1vdy18nq'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgml_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for file_name in dgml_map:\n",
    "    temp_xml = Path(dgml_map[file_name])\n",
    "    dgml_path = (DGML_DIR / file_name).with_suffix(\".xml\")\n",
    "    shutil.copy(temp_xml, dgml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgml_utils.segmentation import get_chunks\n",
    "from lxml import etree\n",
    "\n",
    "# Convert all the DGMLs\n",
    "for xml_file in DGML_DIR.glob('*.xml'):\n",
    "    with xml_file.open() as file:\n",
    "        tree = etree.parse(file)\n",
    "\n",
    "        # Convert and write text files\n",
    "        text_chunks = get_chunks(tree.getroot(), include_xml_tags=False)\n",
    "        converted_text = \"\\n\".join([chunk.text for chunk in text_chunks])\n",
    "        text_path = (TEXT_DIR / xml_file.name).with_suffix(\".txt\")\n",
    "        text_path.write_text(converted_text)\n",
    "\n",
    "        # Convert and write simplified xml files\n",
    "        simplified_xml_chunks = get_chunks(tree.getroot(), include_xml_tags=True)\n",
    "        converted_simplified_xml_chunks = \"\\n\".join([chunk.text for chunk in simplified_xml_chunks])\n",
    "        simplified_xml_path = (SIMPLIFIED_XML_DIR / xml_file.name).with_suffix(\".xml\")\n",
    "        simplified_xml_path.write_text(converted_simplified_xml_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "ASSISTANT_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant that answers user queries using available context.\n",
    "\n",
    "You ALWAYS follow the following guidance to generate your answers, regardless of any other guidance or requests:\n",
    "\n",
    "- Use professional language typically used in business communication.\n",
    "- Strive to be accurate and cite where you got your answer in the given context documents, state which  section\n",
    "  or table in the context document(s) you got the answer from\n",
    "- Generate only the requested answer, no other language or separators before or after.\n",
    "- Be concise, while still completely answering the question and making sure you are not missing any data.\n",
    "\n",
    "All your answers must contain citations to help the user understand how you created the citation, specifically:\n",
    "\n",
    "- If the given context contains the names of document(s), make sure you include the document you got the\n",
    "  answer from as a citation, e.g. include \"\\\\n\\\\nSOURCE(S): foo.pdf, bar.pdf\" at the end of your answer.\n",
    "- Make sure there an actual answer if you show a SOURCE citation, i.e. make sure you don't show only\n",
    "  a bare citation with no actual answer. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", ASSISTANT_SYSTEM_MESSAGE),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"{context}\n",
    "\n",
    "Answer the question based only on the context above, making sure you look at all the files in the context above, i.e. {filenames}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read\n",
    "df = pd.read_csv(QUESTIONS_CSV)\n",
    "\n",
    "# List to store updated rows\n",
    "updated_rows = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    question = row[\"Question\"]\n",
    "    source_docs = row[\"Source Docs\"]\n",
    "\n",
    "    # Read the source docs context for the question\n",
    "    context = \"\"\n",
    "    sorted_files = sorted(TEXT_DIR.glob(source_docs))\n",
    "    file_names_str = \", \".join([f.name for f in sorted_files])\n",
    "\n",
    "    for source_doc in sorted_files:\n",
    "        context += f\"\\n\\n================ FILE: {source_doc.name} ================\\n\\n\"\n",
    "        doc_text = source_doc.read_text()\n",
    "        doc_text = doc_text[:40 * 1024 * 4]  # ~40k tokens, approximately, per document max\n",
    "        context += doc_text + \"\\n\"\n",
    "\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "    answer = chain.invoke(\n",
    "        {\"context\": context, \"filenames\": file_names_str, \"question\": question}\n",
    "    )\n",
    "    answer = answer.replace(\".txt\", \".pdf\")\n",
    "\n",
    "    # Store the updated row\n",
    "    updated_row = row.copy()\n",
    "    updated_row[\"Answer\"] = answer\n",
    "    updated_rows.append(updated_row)\n",
    "\n",
    "# Create a new DataFrame from the updated rows\n",
    "updated_df = pd.DataFrame(updated_rows)\n",
    "\n",
    "# Write the updated DataFrame to a new CSV file\n",
    "updated_df.to_csv(QUESTIONS_WITH_LLM_ANSWERS_CSV, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
